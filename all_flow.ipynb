{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE VARIABLE, FUNCTIONS FOR CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
    "\n",
    "\n",
    "from detect import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init OCR\n",
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['vi','en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init YOLO v7\n",
    "\n",
    "weights     = 'yolov7.pt'\n",
    "source      = 'inference/images'  # file/folder, 0 for webcam\n",
    "img_size    = 640\n",
    "conf_thres  = 0.25  # object confidence threshold\n",
    "iou_thres   = 0.45  # IOU threshold for NMS\n",
    "device      = ''  # cuda device, i.e., '0', '0,1,2,3', or 'cpu'\n",
    "view_img    = False  # display results\n",
    "save_txt    = False  # save results to *.txt\n",
    "save_conf   = False  # save confidences in --save-txt labels\n",
    "nosave      = False  # do not save images/videos\n",
    "classes     = None  # filter by class, e.g., [0] or [0, 2, 3]\n",
    "agnostic_nms = False  # class-agnostic NMS\n",
    "augment     = False  # augmented inference\n",
    "update      = False  # update all models\n",
    "project     = 'runs/detect'  # save results to project/name\n",
    "name        = 'exp'  # save results to project/name\n",
    "exist_ok    = True  # existing project/name ok, do not increment\n",
    "no_trace    = False  # don't trace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR ðŸš€ v0.1-128-ga207844 torch 2.4.0+cu121 CUDA:0 (NVIDIA Graphics Device, 12008.875MB)\n",
      "\n",
      "/home/phucnguyen/code/github_clone/YOLO/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucnguyen/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:811: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  param_grad = param.grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "source, weights, view_img, save_txt, imgsz, trace = source, weights, view_img, save_txt, img_size, not no_trace\n",
    "save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "    ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "# Directories\n",
    "save_dir = Path(increment_path(Path(project) / name, exist_ok=exist_ok))  # increment run\n",
    "(save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "# Initialize\n",
    "set_logging()\n",
    "device = select_device(device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "stride = int(model.stride.max())  # model stride\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "\n",
    "if trace:\n",
    "    model = TracedModel(model, device, img_size)\n",
    "\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "\n",
    "# Second-stage classifier\n",
    "classify = False\n",
    "if classify:\n",
    "    modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "    modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/phucnguyen/code/github_clone/YOLO/yolov7/inference/images/cccd_resized_standard.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALWAYS RESIZE IMAGE TO 1080 X 681 ( AS STANDARD FOR CCCD AND ALSO BEST RESULT IN OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "áº¢nh Ä‘Ã£ resize vÃ  lÆ°u táº¡i: /home/phucnguyen/code/github_clone/YOLO/yolov7/inference/images/cccd_resized_standard.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def resize_image(input_path, output_path):\n",
    "    # Äá»c áº£nh\n",
    "    image = cv2.imread(input_path)\n",
    "    \n",
    "    width = 1080\n",
    "    height = 651\n",
    "\n",
    "    # Resize vá» kÃ­ch thÆ°á»›c má»¥c tiÃªu\n",
    "    resized = cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # LÆ°u áº£nh Ä‘Ã£ resize\n",
    "    cv2.imwrite(output_path, resized)\n",
    "    print(f\"áº¢nh Ä‘Ã£ resize vÃ  lÆ°u táº¡i: {output_path}\")\n",
    "\n",
    "# Resize áº£nh thÃ nh 720p\n",
    "#resize_image(\"data/CCCD.jpg\", \"data/CCCD_resized_720p.jpg\", 1280, 720)\n",
    "\n",
    "# Resize áº£nh thÃ nh 1080p\n",
    "#resize_image(\"data/CCCD.jpg\", \"data/cccd_resized_1080p.jpg\", 1920, 1080)\n",
    "\n",
    "#resize_image(\"data/CCCD.jpg\", \"data/cccd_resized_standard.jpg\", 1280, 807)\n",
    "\n",
    "resize_image(image_path, image_path)  # 1080 x 681\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALL YOLO V7 TO DETECT FACE\n",
    "\n",
    "After this, we will have bounding box of human or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to hold\n",
    "human = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phucnguyen/.local/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Set Dataloader\n",
    "vid_path, vid_writer = None, None\n",
    "if webcam:\n",
    "    view_img = check_imshow()\n",
    "    cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "    dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "else:\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "# Get names and colors\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "\n",
    "# Run inference\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "old_img_w = old_img_h = imgsz\n",
    "old_img_b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 person, \n",
      "1 person, 2 books, \n",
      "Image size (im0): (681, 1080, 3)\n",
      "Image size (im0): (681, 1080, 3)\n",
      "Image size (im0): (681, 1080, 3)\n",
      "Found Human\n",
      "Done. (0.534s)\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for path, img, im0s, vid_cap in dataset:\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Warmup\n",
    "    if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
    "        old_img_b = img.shape[0]\n",
    "        old_img_h = img.shape[2]\n",
    "        old_img_w = img.shape[3]\n",
    "        for i in range(3):\n",
    "            model(img, augment=augment)[0]\n",
    "\n",
    "    # Inference\n",
    "    t1 = time_synchronized()\n",
    "    with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
    "        pred = model(img, augment=augment)[0]\n",
    "    t2 = time_synchronized()\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, conf_thres, iou_thres, classes=classes, agnostic=agnostic_nms)\n",
    "    t3 = time_synchronized()\n",
    "\n",
    "    # Apply Classifier\n",
    "    if classify:\n",
    "        pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if human == True:\n",
    "            break\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "        else:\n",
    "            p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "        p = Path(p)  # to Path\n",
    "        save_path = str(save_dir / p.name)  # img.jpg\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # Print results\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # detections per class\n",
    "                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "                \n",
    "                print(s)\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                \n",
    "                print(f\"Image size (im0): {im0.shape}\")\n",
    "                if save_txt:  # Write to file\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                    with open(txt_path + '.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or view_img:  # Add bbox to image\n",
    "                    label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                    \n",
    "                    if names[int(cls)] != \"person\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        face_bounding_box = [[int(xyxy[0]), int(xyxy[1])], [int(xyxy[2]), int(xyxy[3])]]\n",
    "                        human = True\n",
    "                        print(\"Found Human\")\n",
    "                        break\n",
    "                        #plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n",
    "                    \n",
    "\n",
    "        # Print time (inference + NMS)\n",
    "#         print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
    "\n",
    "#         # Stream results\n",
    "#         if view_img:\n",
    "#             cv2.imshow(str(p), im0)\n",
    "#             cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "#         # Save results (image with detections)\n",
    "#         if save_img:\n",
    "#             if dataset.mode == 'image':\n",
    "#                 cv2.imwrite(save_path, im0)\n",
    "#                 print(f\" The image with the result is saved in: {save_path}\")\n",
    "#             else:  # 'video' or 'stream'\n",
    "#                 if vid_path != save_path:  # new video\n",
    "#                     vid_path = save_path\n",
    "#                     if isinstance(vid_writer, cv2.VideoWriter):\n",
    "#                         vid_writer.release()  # release previous video writer\n",
    "#                     if vid_cap:  # video\n",
    "#                         fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "#                         w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#                         h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#                     else:  # stream\n",
    "#                         fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "#                         save_path += '.mp4'\n",
    "#                     vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "#                 vid_writer.write(im0)\n",
    "\n",
    "# if save_txt or save_img:\n",
    "#     s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "#     #print(f\"Results saved to {save_dir}{s}\")\n",
    "\n",
    "print(f'Done. ({time.time() - t0:.3f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov7.pt'], source='inference/images/cccd_resized_standard.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
      "YOLOR ðŸš€ v0.1-128-ga207844 torch 2.4.0+cu121 CUDA:0 (NVIDIA Graphics Device, 12008.875MB)\n",
      "\n",
      "/home/phucnguyen/code/github_clone/YOLO/yolov7/models/experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "Model Summary: 306 layers, 36905341 parameters, 6652669 gradients\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "/home/phucnguyen/.local/lib/python3.10/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "1 person, \n",
      "1 person, 1 book, \n",
      "Image size (im0): (651, 1080, 3)\n",
      "bounding box:  (46, 239) (301, 571)\n",
      "bounding box:  (46, 239) (100, 228)\n",
      "Image size (im0): (651, 1080, 3)\n",
      "bounding box:  (49, 243) (297, 570)\n",
      "bounding box:  (49, 243) (114, 232)\n",
      "1 person, 1 book, Done. (7.2ms) Inference, (732.1ms) NMS\n",
      " The image with the result is saved in: runs/detect/exp14/cccd_resized_standard.jpg\n",
      "Done. (1.047s)\n"
     ]
    }
   ],
   "source": [
    "#!python detect.py --weights yolov7.pt --conf 0.25  --source inference/images/cccd_resized_standard.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if human != True:\n",
    "    print(\"Else\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF WE HAVE THE BOUNDING BOX, BASE ON THE FIX TEMPLATE OF CCCD, WE CAN APPROXIMATE THE AREA WHICH HAVE BASED INFORMATION OF CCCD\n",
    "\n",
    "THEN WE WILL CUT THIS AREA OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_size = [1080, 681]\n",
    "\n",
    "face_bounding_box = face_bounding_box\n",
    "\n",
    "face_height = face_bounding_box[1][1] - face_bounding_box[0][1]\n",
    "face_width = face_bounding_box[1][0] - face_bounding_box[0][0]\n",
    "\n",
    "height_position_percent = 0.73\n",
    "\n",
    "width_percent_differ = 2.25\n",
    "height_percent_differ = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if face_height > 420 or face_width > 390:\n",
    "    print(\"Human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def calculate_wanted_area(fix_image_size, face_bounding_box, height_position_percent, width_percent_differ, height_percent_differ, face_height, face_width):\n",
    "text_bounding_box = [[0,0], [0,0]]\n",
    "\n",
    "text_bounding_box[1][0] = face_bounding_box[1][0] + face_width * width_percent_differ\n",
    "\n",
    "text_bounding_box[1][1] = face_bounding_box[1][1] - face_height * height_position_percent\n",
    "\n",
    "text_bounding_box[0][0] = face_bounding_box[1][0]\n",
    "text_bounding_box[0][1] = text_bounding_box[1][1] - face_height * height_percent_differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All corners: [[312, 47.160000000000025], [847.5, 47.160000000000025], [847.5, 344.51], [312, 344.51]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract the coordinates\n",
    "top_left = text_bounding_box[0]\n",
    "bottom_right = text_bounding_box[1]\n",
    "\n",
    "# Calculate the other two corners\n",
    "top_right = [bottom_right[0], top_left[1]]  # [c, b]\n",
    "bottom_left = [top_left[0], bottom_right[1]]  # [a, d]\n",
    "\n",
    "# Combine all four corners\n",
    "all_corners = [top_left, top_right, bottom_right, bottom_left]\n",
    "\n",
    "# Print the result\n",
    "print(\"All corners:\", all_corners)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Error: Could not load image.\")\n",
    "    exit()\n",
    "\n",
    "# Function to crop regions based on bounding box\n",
    "output_dir = \"crop/output_regions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Convert points to numpy array\n",
    "points_array = np.array(all_corners, dtype=np.int32)\n",
    "\n",
    "# Get bounding rectangle (x_min, y_min, width, height)\n",
    "x, y, w, h = cv2.boundingRect(points_array)\n",
    "\n",
    "# Crop the region\n",
    "cropped = image[y:y+h, x:x+w]\n",
    "\n",
    "# Save the cropped region\n",
    "output_path = f\"{output_dir}/cut.jpg\"\n",
    "cv2.imwrite(output_path, cropped)\n",
    "#print(f\"Saved region '{name}' to {output_path}\")\n",
    "\n",
    "\n",
    "img = Image.open(output_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Optionally hide axes\n",
    "plt.show()  # This will display the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCR WITH EASYOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CONG HOA XA HOI CHU NGHIA VIET NAM',\n",
       " 'Äá»™c lÃªP',\n",
       " 'Tá»± do',\n",
       " 'Hanh phÃºc',\n",
       " 'SOCIALIST REPUBLIC OF VIET NAM',\n",
       " 'Independence',\n",
       " 'Freedom',\n",
       " 'Happiness',\n",
       " 'CÄ‚N cÆ¯á»šC CÃ”NG DÃ‚N',\n",
       " 'Citizen Identity Card',\n",
       " 'SÃ³ / No :',\n",
       " '048202007174']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = reader.readtext('crop/output_regions/cut.jpg', detail = 0)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE BASE INFORMATION ON CCCD\n",
    "\n",
    "IF MATCH SOME OF THESE, THIS WILL VERY LIKELY A CCCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_define_CCCD = ['conghoaxahoichunghiavietnam',\n",
    " 'doclap',\n",
    " 'tudo',\n",
    " 'hanhphuc',\n",
    " 'socialistrepublicofvietnam',\n",
    " 'independence',\n",
    " 'freedom',\n",
    " 'happiness',\n",
    " 'cancuocongdan',\n",
    " 'citizenidentitycard',\n",
    " 'so/no']\n",
    "\n",
    "total_check = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conghoaxahoichunghiavietnam', 'doclep', 'tudo', 'hanhphuc', 'socialistrepublicofvietnam', 'independence', 'freedom', 'happiness', 'cancuoccongdan', 'citizenidentitycard', 'sono', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Function to normalize text\n",
    "def normalize_vietnamese(text):\n",
    "    # Remove accents using unidecode\n",
    "    text = unidecode(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Remove extra spaces\n",
    "    text = \" \".join(text.split())\n",
    "    text = text.replace(\" \", \"\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Process the list\n",
    "normalized_output = [normalize_vietnamese(item) for item in result]\n",
    "\n",
    "# Print the result\n",
    "print(normalized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day la CCCD\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in normalized_output:\n",
    "    if word in pre_define_CCCD:\n",
    "        count +=1\n",
    "        \n",
    "if count/11 > 0.5:\n",
    "    print(\"Day la CCCD\")\n",
    "else:\n",
    "    print('Day la Else')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
